{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper details:\n",
    "Title: Translating Embeddings for Modeling Multi-relational Data  \n",
    "Authors: Antoine Bordes, Nicolas Usunier, Alberto Garcia-Durán  \n",
    "Venue: NIPS-2013  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Q&A\n",
    "**0. What is the task and it\"s real world significance?**  \n",
    "Link Prediction is a task in graph and network analysis where the goal is to predict missing or future connections between nodes in a network. Given a partially observed network, the goal of link prediction is to infer which links are most likely to be added or missing based on the observed connections and the structure of the network. Examples of link prediction include predicting friendship links among users in a social network, predicting co-authorship links in a citation network, and predicting interactions between genes and proteins in a biological network.  \n",
    "\n",
    "**1. How do you ensure that entities and relationship gets the embeddings that are close to reality given the data?**  \n",
    "TransE, an energy-based model for learning low-dimensional embeddings of entities. In TransE, relationships\n",
    "are represented as translations in the embedding space: if (h, l, t) holds, then the embedding of the\n",
    "tail entity t should be close to the embedding of the head entity h plus some vector that depends\n",
    "on the relationship l. Our approach relies on a reduced set of parameters as it learns only one\n",
    "low-dimensional vector for each entity and each relationship.  \n",
    "\n",
    "**2. What are the previous inspirations that lead to this framing of transE.**  \n",
    "The main motivation behind our translation-based parameterization is that hierarchical relationships\n",
    "are extremely common in KBs and translations are the natural transformations for representing them. Another, secondary, motivation comes from the recent work of word2vec, in which the authors learn word embeddings from free text, and some 1-to-1 relationships between entities of different types, such “capital of” between countries and cities, are (coincidentally rather than willingly) represented by the model as translations in the embedding space. This suggests that there may exist embedding spaces in which 1-to-1 relationships between entities of different types may, as well, be represented by translations.\n",
    "\n",
    "**3. What are the emperically found hyper parameters that produced best results?**  \n",
    "For experiments with TransE, we selected the learning rate λ for the stochastic\n",
    "gradient descent among {0.001, 0.01, 0.1}, the margin γ among {1, 2, 10} and the latent dimension\n",
    "k among {20, 50} on the validation set of each data set. The dissimilarity measure d was set either\n",
    "to the L1 or L2 distance according to validation performance as well. Optimal configurations were:\n",
    "k = 20, λ = 0.01, γ = 2, and d = L1 on Wordnet; k = 50, λ = 0.01, γ = 1, and d = L 1 on FB15k; k = 50, λ = 0.01, γ = 1, and d = L2 on FB1M. For all data sets, training time was limited to at most 1, 000 epochs over the training set.  \n",
    "**4. What is the loss function selected. Why not anything else?**  \n",
    "For training for each triplet (head, relation, tree) a corrupted triplet ids generated (head', relation, tail') by replacing either head or tail but not both from the original triplet. Loss function is designed in such a way the it favours d(head+relation, tail) more than  d(head'+relation, tail') where d can be any distance measure like L1-norm, or L2-norm. So choice of loss function is Margin Ranking loss.  \n",
    "**5. How do you evaluate performance?**   \n",
    "hits@10, mean rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.auto import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are running a kaggle notebook\n",
    "if os.environ.get('KAGGLE_URL_BASE',''):\n",
    "    DATA_DIR = \"/kaggle/input\"\n",
    "    OUTPUT_DIR = \"/kaggle/output\"\n",
    "else:\n",
    "    os.makedirs(\"input/kg_embeddings\", exist_ok=True)\n",
    "    os.makedirs(\"output/kg_embeddings\", exist_ok=True)\n",
    "    # # download files from kaggle\n",
    "    #!kaggle datasets download -d latebloomer/fb15k-237\n",
    "    #!unzip -d input/kg_embeddings fb15k-237.zip\n",
    "    DATA_DIR = \"input/kg_embeddings\"\n",
    "    OUTPUT_DIR = \"output/kg_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/kg_embeddings/valid.txt\n",
      "input/kg_embeddings/test.txt\n",
      "input/kg_embeddings/train.txt\n",
      "input/kg_embeddings/.ipynb_checkpoints/test-checkpoint.txt\n",
      "input/kg_embeddings/.ipynb_checkpoints/train-checkpoint.txt\n",
      "input/kg_embeddings/.ipynb_checkpoints/valid-checkpoint.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk(DATA_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = get_device()\n",
    "print(f\"device: {device}\")\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data exploration\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/train.txt\", delimiter=\"\\t\", names=[\"head\", \"relation\", \"tail\"])\n",
    "test_df = pd.read_csv(f\"{DATA_DIR}/test.txt\", delimiter=\"\\t\", names=[\"head\", \"relation\", \"tail\"])\n",
    "val_df = pd.read_csv(f\"{DATA_DIR}/valid.txt\", delimiter=\"\\t\", names=[\"head\", \"relation\", \"tail\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(entities): 14541\n",
      "len(relation): 237\n"
     ]
    }
   ],
   "source": [
    "# generate vocabulary\n",
    "entities = pd.concat(\n",
    "    [train_df[\"head\"], test_df[\"head\"], val_df[\"head\"], train_df[\"tail\"], test_df[\"tail\"], val_df[\"tail\"]], axis=0)\n",
    "entities = sorted(list(set(entities)))\n",
    "print(f\"len(entities): {len(entities)}\")\n",
    "relations = pd.concat([train_df[\"relation\"], test_df[\"relation\"], val_df[\"relation\"]])\n",
    "relations = sorted(list(set(relations)))\n",
    "\n",
    "print(f\"len(relation): {len(relations)}\")\n",
    "\n",
    "id2word = entities + relations\n",
    "word2id = {word: i for i, word in enumerate(id2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize embedding layer\n",
    "\n",
    "def relation_data_loader(df):\n",
    "    \"\"\"\n",
    "    takes dataframe\n",
    "    returns: one positive sample(triple) and one negative sample\n",
    "    \"\"\"\n",
    "    df = shuffle(df)\n",
    "    for i, row in df.iterrows():\n",
    "        positive_example = [word2id[item] for item in row]\n",
    "        # randomly sample negative example by replacing head or tail with random entity\n",
    "        #   with probability 0.5\n",
    "        if np.random.rand() > 0.5:\n",
    "            negative_example = [word2id[item] for item in row]\n",
    "            negative_example[0] = word2id[np.random.choice(entities)]\n",
    "        else:\n",
    "            negative_example = [word2id[item] for item in row]\n",
    "            negative_example[2] = word2id[np.random.choice(entities)]\n",
    "        yield positive_example, negative_example\n",
    "\n",
    "\n",
    "def distance_function(t1, t2):\n",
    "    # implement L1 norm as distance function\n",
    "    return torch.sum(torch.abs(t1 - t2), dim=-1)\n",
    "\n",
    "\n",
    "# validation code\n",
    "def test_accuracy(val_df):\n",
    "    total = val_df.shape[0]\n",
    "    correct = 0\n",
    "    for i, (positive_sample, *_) in tqdm(enumerate(relation_data_loader(val_df)), total=val_df.shape[0]):\n",
    "        head, relation, actual_entity = positive_sample\n",
    "        entity_relation_vector = embed_layer(to_device(torch.LongTensor([head, relation]), device))\n",
    "        # print(entity_relation_vector.shape)\n",
    "        pred_vector = entity_relation_vector[0] + entity_relation_vector[1]\n",
    "        # print(pred_vector.shape)\n",
    "        closest_entity = torch.argmin(distance_function(pred_vector, embed_layer.weight.data), dim=-1)\n",
    "        # print(f\"closest_entity: {closest_entity}\")\n",
    "        if closest_entity == actual_entity:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Stochastic gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T00:51:53.230360862Z",
     "start_time": "2024-02-26T00:51:53.093916966Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# hyper parameters\n",
    "# k = 50, λ = 0.01, γ = 1, and d = L1 on FB15k;\n",
    "embedding_dim = 50\n",
    "lr = 0.01\n",
    "gamma = 1\n",
    "epoch = 1000\n",
    "\n",
    "# define layer\n",
    "embed_layer = nn.Embedding(len(id2word), embedding_dim)\n",
    "\n",
    "# input = torch.LongTensor([[0, 2, 0, 5]])\n",
    "# embed_layer(input)\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    for i, (positive_sample, negative_sample) in tqdm(enumerate(relation_data_loader(train_df)),\n",
    "                                                      total=train_df.shape[0]):\n",
    "        positive_sample = torch.LongTensor(positive_sample)\n",
    "        negative_sample = torch.LongTensor(negative_sample)\n",
    "        positive_input = embed_layer(positive_sample)\n",
    "        negative_input = embed_layer(negative_sample)\n",
    "        # calculate distance\n",
    "        positive_distance = distance_function(positive_input[0] + positive_input[1], positive_input[2])\n",
    "        negative_distance = distance_function(negative_input[0] + negative_input[1], negative_input[2])\n",
    "        # calculate loss\n",
    "        loss = gamma + positive_distance - negative_distance\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        embed_layer.weight.data -= lr * embed_layer.weight.grad.data\n",
    "    # if e%10==0:\n",
    "    accuracy = test_accuracy(val_df)\n",
    "    print(f\"accuracy @epoch{e}: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyper parameters\n",
    "# k = 50, λ = 0.01, γ = 1, and d = L1 on FB15k;\n",
    "embedding_dim = 50\n",
    "lr = 0.01\n",
    "gamma = 1\n",
    "epoch = 1000\n",
    "\n",
    "# define layer\n",
    "embed_layer = nn.Embedding(len(id2word), embedding_dim)\n",
    "\n",
    "# input = torch.LongTensor([[0, 2, 0, 5]])\n",
    "# embed_layer(input)\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    for i, (positive_sample, negative_sample) in tqdm(enumerate(relation_data_loader(train_df)),\n",
    "                                                      total=train_df.shape[0]):\n",
    "        positive_sample = torch.LongTensor(positive_sample)\n",
    "        negative_sample = torch.LongTensor(negative_sample)\n",
    "        positive_input = embed_layer(positive_sample)\n",
    "        negative_input = embed_layer(negative_sample)\n",
    "        # calculate distance\n",
    "        positive_distance = distance_function(positive_input[0] + positive_input[1], positive_input[2])\n",
    "        negative_distance = distance_function(negative_input[0] + negative_input[1], negative_input[2])\n",
    "        # calculate loss\n",
    "        loss = gamma + positive_distance - negative_distance\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        embed_layer.weight.data -= lr * embed_layer.weight.grad.data\n",
    "    # if e%10==0:\n",
    "    accuracy = test_accuracy(val_df)\n",
    "    print(f\"accuracy @epoch{e}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch neat implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Where is negative sample formed?\n",
    "# 2. How is loss applied?\n",
    "# 3. How does bulk processing happens?\n",
    "# 4. What are the customized hyperparameters \n",
    "\n",
    "def data_loader(df, batch_size=1000):\n",
    "    \"\"\"\n",
    "    takes dataframe\n",
    "    returns: one positive sample(triple) and one negative sample\n",
    "    \"\"\"\n",
    "    df = shuffle(df)\n",
    "    # yield 3 series of head, relation and tail of batch_size\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        heads = batch_df[\"head\"].apply(lambda x: word2id[x])\n",
    "        tails = batch_df[\"tail\"].apply(lambda x: word2id[x])\n",
    "        relations = batch_df[\"relation\"].apply(lambda x: word2id[x])\n",
    "        yield to_device(torch.tensor(heads.values), device), to_device(torch.tensor(relations.values), device), to_device(torch.tensor(tails.values), device)\n",
    "\n",
    "\n",
    "class transE(torch.nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim=50, p_norm=1, margin=1):\n",
    "        super(transE, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_entities = num_entities\n",
    "        self.num_relations = num_relations\n",
    "        self.p_norm = p_norm\n",
    "        self.margin = margin\n",
    "        self.embedding = torch.nn.Embedding(num_entities+num_relations, embedding_dim)\n",
    "        # self.relation_embedding = torch.nn.Embedding(num_relations, embedding_dim)\n",
    "\n",
    "    def forward(self, head_idx, relation_idx):\n",
    "        # head_relation_idx = torch.LongTensor([head, relation])\n",
    "        # print(f\"head_idx.shape:{head_idx.shape}\")\n",
    "        # print(f\"tail_idx.shape:{relation_idx.shape}\")\n",
    "        head_vectors = self.embedding(head_idx)\n",
    "        relation_vectors = self.embedding(relation_idx)\n",
    "        # print(f\"head_vectors.shape: {head_vectors.shape}\")\n",
    "        # print(f\"relation_vectors.shape: {relation_vectors.shape}\")\n",
    "        \n",
    "        # normalize head\n",
    "        head_vectors = F.normalize(head_vectors, p=self.p_norm, dim=-1)\n",
    "        return head_vectors + relation_vectors\n",
    "    \n",
    "    def loss(self, head_idx, relation_idx, tail_idx):\n",
    "        \n",
    "        # calculate loss on positive sample\n",
    "        # triplet_tensor = torch.LongTensor(triplet)\n",
    "        positive_prediction = self(head_idx, relation_idx) # head, relation\n",
    "        # print(f\"positive_prediction.shape:{positive_prediction.shape}\")\n",
    "        actual_tail = self.embedding(tail_idx)\n",
    "        # print(f\"actual_tail.shape:{actual_tail.shape}\")\n",
    "        pos_score = -(positive_prediction - actual_tail).norm(p=self.p_norm, dim=-1)\n",
    "        # print(f\"pos_score.shape: {pos_score.shape}\")\n",
    "        # corrupt the triplet by replacing either head or tail but not both with probability 0.5\n",
    "        # create mask where head is going to be replaced\n",
    "        mask = to_device(torch.rand(head_idx.shape) > 0.5, device)\n",
    "        # create negative sample\n",
    "        corrupt_head_idx = head_idx.clone()\n",
    "        corrupt_tail_idx = tail_idx.clone()\n",
    "        corrupt_head_idx[mask] = to_device(torch.randint(0, self.num_entities, corrupt_head_idx[mask].shape), device)\n",
    "        corrupt_tail_idx[~mask] = to_device(torch.randint(0, self.num_entities, corrupt_tail_idx[~mask].shape), device)\n",
    "        # calculate loss on negative sample\n",
    "        corrupt_prediction = self(corrupt_head_idx, relation_idx)\n",
    "        corrupt_tail = self.embedding(corrupt_tail_idx)\n",
    "        neg_score = -(corrupt_prediction - corrupt_tail).norm(p=self.p_norm, dim=-1)\n",
    "        # print(f\"neg_score.shape: {neg_score.shape}\")\n",
    "        \n",
    "        return F.margin_ranking_loss(\n",
    "            pos_score,\n",
    "            neg_score,\n",
    "            target=torch.ones_like(pos_score),\n",
    "            margin=self.margin,\n",
    "        )\n",
    "\n",
    "        # return torch.max(positive_score - corrupt_score + 1, torch.zeros(1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81af5ef6fcc45dbaeb18bac038eaa37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_loss@0: 1.3288127682957833\n",
      "mean_loss@100: 0.19604749704059815\n",
      "mean_loss@200: 0.17711706547056952\n",
      "mean_loss@300: 0.1746158214719981\n",
      "mean_loss@400: 0.17049867751736442\n",
      "mean_loss@500: 0.16694723726657257\n",
      "mean_loss@600: 0.16342769713022895\n",
      "mean_loss@700: 0.16230817977498177\n",
      "mean_loss@800: 0.16200405639559953\n",
      "mean_loss@900: 0.1582687585944126\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "num_entities= len(entities)\n",
    "num_relations = len(relations)\n",
    "embedding_dim=50\n",
    "lr=0.01\n",
    "epoch = 1000\n",
    "p_norm=1\n",
    "margin=1\n",
    "batch_size=1000\n",
    "\n",
    "# define model\n",
    "model = transE(num_entities, num_relations, embedding_dim, p_norm=p_norm, margin=margin)\n",
    "\n",
    "model = to_device(model, device)\n",
    "# define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = total_examples = 0\n",
    "    for head_index, rel_type, tail_index in data_loader(train_df, batch_size=batch_size):\n",
    "        # print(head_index, rel_type, tail_index)\n",
    "        # break\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(head_index, rel_type, tail_index)\n",
    "        # print(loss)\n",
    "        # break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * head_index.numel()\n",
    "        total_examples += head_index.numel()\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "for epoch_i in tqdm(range(epoch)):\n",
    "    mean_loss = train()\n",
    "    if epoch_i%100 ==0:\n",
    "        print(f\"mean_loss@{epoch_i}: {mean_loss}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), f\"{OUTPUT_DIR}/transE_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy:0.01699458226404334\n",
      "test_accuracy:0.016466334408286914\n",
      "test_hit_at_10:0.2267663441805922\n",
      "test_hit_at_20:0.3146193687090785\n",
      "test_hit_at_30:0.37256913905990424\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "saved_model = transE(num_entities, num_relations, embedding_dim, p_norm=p_norm, margin=margin)\n",
    "saved_model = to_device(saved_model, device)\n",
    "saved_model.load_state_dict(torch.load(f\"{OUTPUT_DIR}/transE_model.pth\"))\n",
    "\n",
    "# inference related code\n",
    "def test_accuracy(val_df):\n",
    "    model.eval()\n",
    "    total = val_df.shape[0]\n",
    "    correct = 0\n",
    "    for head_index, rel_type, tail_index in data_loader(val_df, batch_size=batch_size):\n",
    "        head_vectors = saved_model.embedding(head_index)\n",
    "        relation_vectors = saved_model.embedding(rel_type)\n",
    "        pred_vectors = head_vectors + relation_vectors\n",
    "        closest_entity = torch.argmin((pred_vectors.unsqueeze(1) - model.embedding.weight.data).norm(p=1, dim=-1), dim=-1)\n",
    "        correct += (closest_entity == tail_index).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def test_hits(val_df, k=10):\n",
    "    model.eval()\n",
    "    total = val_df.shape[0]\n",
    "    correct = 0\n",
    "    for head_index, rel_type, tail_index in data_loader(val_df, batch_size=batch_size):\n",
    "        head_vectors = saved_model.embedding(head_index)\n",
    "        relation_vectors = saved_model.embedding(rel_type)\n",
    "        pred_vectors = head_vectors + relation_vectors\n",
    "        closest_entities = (pred_vectors.unsqueeze(1) - model.embedding.weight.data).norm(p=1, dim=-1).argsort(dim=-1)[:,:k]\n",
    "        correct += (closest_entities == tail_index.unsqueeze(1)).sum().sum().item()\n",
    "    return correct / total\n",
    "\n",
    "    \n",
    "# accuracy\n",
    "val_accuracy = test_accuracy(val_df)\n",
    "print(f\"val_accuracy:{val_accuracy}\")\n",
    "test_accuracy = test_accuracy(test_df)\n",
    "print(f\"test_accuracy:{test_accuracy}\")\n",
    "\n",
    "\n",
    "# hits\n",
    "test_hit_at_10  = test_hits(test_df, k=10)\n",
    "print(f\"test_hit_at_10:{test_hit_at_10}\")\n",
    "test_hit_at_20  = test_hits(test_df, k=20)\n",
    "print(f\"test_hit_at_20:{test_hit_at_20}\")\n",
    "test_hit_at_30  = test_hits(test_df, k=30)\n",
    "print(f\"test_hit_at_30:{test_hit_at_30}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. [Translating Embeddings for Modeling\n",
    "Multi-relational Data(Paper)](https://papers.nips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf)\n",
    "2. [Link prediction - papers with code](https://paperswithcode.com/task/link-prediction)\n",
    "3. [Link prediction - Wikipedia](https://en.wikipedia.org/wiki/Link_prediction)\n",
    "4. [MARGIN RANKING LOSS - Pytorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.MarginRankingLoss.html)\n",
    "5. [Understanding Ranking Loss, Contrastive Loss, Margin Loss, Triplet Loss, Hinge Loss and all those confusing names](https://gombru.github.io/2019/04/03/ranking_loss/)\n",
    "6. [pytorch geometric - TransE](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.TransE.html#torch_geometric.nn.kge.TransE)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4488713,
     "sourceId": 7691439,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
